import br.uff.ic.domain.Coupling
import br.uff.ic.domain.Project
import br.uff.ic.logger.ConsoleHandler
import br.uff.ic.logger.Logger
import br.uff.ic.logger.LoggerFactory
import br.uff.ic.mining.DataSet
import br.uff.ic.mining.Transaction
import br.uff.ic.mining.Rule
import br.uff.ic.pipelines.JsonBucket
import com.xenomachina.argparser.ArgParser
import com.xenomachina.argparser.default
import io.netty.util.internal.ConcurrentSet
import kotlinx.coroutines.experimental.runBlocking
import org.apache.spark.SparkConf
import org.apache.spark.api.java.JavaSparkContext
import org.apache.spark.mllib.fpm.FPGrowth
import org.apache.spark.mllib.fpm.FPGrowthModel
import java.io.File
import java.nio.file.Files
import java.nio.file.Path
import java.nio.file.Paths
import kotlin.streams.toList
import kotlin.system.measureTimeMillis


object ImportMining {

    val sparkContext: JavaSparkContext
    private val tempDirectory : String
    private val logger : Logger

    init {
        LoggerFactory.addHandler(ConsoleHandler())
        val sparkConf = SparkConf()
                .setAppName(APP_NAME)
                .setMaster("local[8]")
                .set("spark.executor.memory", "1g")
        sparkContext = JavaSparkContext(sparkConf)
        tempDirectory = "$LOCATION\\temp"
        logger = LoggerFactory.new(ImportMining::class.java)
    }

    @Suppress("JoinDeclarationAndAssignment")
    @JvmStatic
    fun main(args: Array<String>) {
        val arguments = ArgParser(args)

        val bucketLocation by arguments.storing("-l", "--location", help ="set output location")
                                        .default(LOCATION)
        val repositoryUri by arguments.storing("-u", "--uri", help = "set git repository URI")
                                        .default(URI)
        val deleteAtTheEnd by arguments.flagging("-d", "--delete", help = "should the project files analyzed be deleted after the execution?")
                                        .default(false)

        measureTimeMillis {
            val bucket = JsonBucket(bucketLocation)
            val project : Project
            val dataSet : DataSet
            val rules : Collection<Rule>
            val couplings : Collection<Coupling>
            val minimumSupport : Double

            logger.info("cloning the repository: $repositoryUri @ $tempDirectory")
            project = cloneRepository(tempDirectory, repositoryUri)
            //minimumSupport = project.sourcePaths.size.let { if(it > 10) 10.0 / it else BASE_SUPPORT }
            minimumSupport = BASE_SUPPORT
            logger.info("collecting imports information")
            dataSet = collectImports(project)
            logger.info("learning association rules from imports information")
            rules = learnAssociationRules(dataSet, minimumSupport, BASE_CONFIDENCE)
            bucket.save("extracted-rules", rules)
            logger.info("measuring coupling from rules")
            couplings = measureCouplingInformation(rules)
            bucket.save("couplings-found", couplings)

            if(deleteAtTheEnd)
                File(tempDirectory)
                        .listFiles()
                        .forEach { it.deleteRecursively() }

            // TODO: analyze possbile metrics based on coupling values generated by rules
        }.apply {
            logger.info("${toDouble() / 1000}")
        }
    }

    fun cloneRepository(folderLocation : String, repositoryURI : String) : Project {
        val directory = File(folderLocation)

        if(directory.listFiles().count() <= 1){
            val cmd = "git clone --depth=1 $repositoryURI ${directory.absolutePath}"
            val status = Runtime.getRuntime()
                    .exec(cmd)
                    .waitFor()
            if (status != 0) {
                throw Exception("Could not clone the repository: $repositoryURI. Status=$status")
            }
        }

        return Project(directory, listOf(), listOf(), listOf(), listOf())
    }

    fun collectImports(project : Project) : DataSet {

        project.listMainSourcePaths()
                .parseSourceFiles()
                .listPackages()
                .removeExternalImports()
                .findLocalImports()
                .let {
                    val rows = it.sourceFiles.parallelStream().map { Transaction(it.getFilePath(), it.imports)}.toList()
                    val header = it.imports.sorted()
                    return DataSet(header, rows)
                }
    }

    fun learnAssociationRules(dataSet: DataSet, minimumSupport : Double, minimumConfidence : Double) : Collection<Rule> {
        FPGrowth()
        .setMinSupport(minimumSupport)
        .setNumPartitions(10)
        .run(dataSet.toRDD())
        .generateAssociationRules(minimumConfidence)
        .toLocalIterator()
        .toIterable()
        .let {
            val rules = mutableListOf<Rule>()
            for (rule in it) {
                rules.add(Rule.fromSparkRule(rule, dataSet))
            }
            return rules
        }
    }

    fun measureCouplingInformation(rules : Collection<Rule>) : Collection<Coupling>{
        return rules.flatMap { it.items }
                .flatMap { item ->
                    rules.filter { it.items.contains(item) }
                         .map { it.removeItem(item) }
                         .flatMap { it.items.map { item to it } }
                }
                .map { pair -> Coupling(pair.first, pair.second, rules.filter { it.items.containsAll(pair.toList()) }.map { it.removeInstances() }) }
        // TODO: testar que todas as regras que contém estes itens estão no Acoplamento devido
        // TODO remover duplicatas  do tipo A->B, B->A e duplicatas exatas

    }
}